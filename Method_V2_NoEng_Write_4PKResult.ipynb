{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [二版] - 去 英數字 符號 逗號 , 且可選是否印標題\n",
    "# 實作 - 寫入檔案\n",
    "# 萃取字詞的四大方法\n",
    "# 首先要先確定有 [字典] 和 [停用詞] 的txt檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\BigData\\dict.big.tra.txt ...\n",
      "DEBUG:jieba:Building prefix dict from C:\\Users\\BigData\\dict.big.tra.txt ...\n",
      "Loading model from cache c:\\users\\bigdata\\appdata\\local\\temp\\jieba.ud8ec30fabaf60e161f06b3552aab2f0e.cache\n",
      "DEBUG:jieba:Loading model from cache c:\\users\\bigdata\\appdata\\local\\temp\\jieba.ud8ec30fabaf60e161f06b3552aab2f0e.cache\n",
      "Loading model cost 0.266 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.266 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg  \n",
    "jieba.set_dictionary('dict.big.tra.txt')  #預設字典\n",
    "jieba.load_userdict('dict_twstd_tfidf.txt')  #中文分詞詞庫(TFIDF)\n",
    "jieba.load_userdict('dict_ntusd_pos.txt') #NTUSD_負向\n",
    "jieba.load_userdict('dict_ntusd_nag.txt') #NTUSD_正向\n",
    "jieba.load_userdict('dict_twedu_dict.txt') #教育部《重編國語辭典 修訂本》\n",
    "\n",
    "\n",
    "#jieba.load_userdict('dict_20160111_NotReallyUseful.txt')  \n",
    "#中文分詞詞庫(TFIDF) + 百度分詞詞庫 + 69萬中文大辭典 + 中文热门词库(155073) \n",
    "#不是很好用，可備著使用\n",
    "\n",
    "\n",
    "jieba.analyse.set_stop_words(\"stopwords_0104_test.txt\") #停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#法一\n",
    "def jiebaNoEng(attraction,rankNum,filename,title):\n",
    "    import re\n",
    "    f = open(attraction,'r')  #讀取檔案\n",
    "    f1 = f.read()  #接住\n",
    "    f2 = ''.join(f1.split())  #把空白全去掉\n",
    "    words=jieba.cut(f2)  #斷詞\n",
    "    \n",
    "    li = []\n",
    "    for u in words:\n",
    "        if not re.search('(\\w)(\\w)',u):\n",
    "            li.append(u)\n",
    "    \n",
    "    n1 = (' '.join(li)).encode('utf-8')\n",
    "    f.close()\n",
    "    x = jieba.analyse.extract_tags(n1, topK=rankNum, withWeight=True)    \n",
    "    fid = open(filename,'a')\n",
    "    \n",
    "    #*#*#*#*#*#*#*#*#*#\n",
    "    #決定是否要印景點標題#\n",
    "\n",
    "    #print attraction   \n",
    "    #fid.write(attraction)\n",
    "    #fid.write('\\n')\n",
    "    \n",
    "    TitleDic=['True','T','t','Yes','Y','y']\n",
    "    if title in TitleDic:\n",
    "        fid.write(attraction)\n",
    "        fid.write('\\n')\n",
    "    else:\n",
    "        fid.write('')\n",
    "        \n",
    "    #*#*#*#*#*#*#*#*#*#\n",
    "    \n",
    "    for i in x:\n",
    "        fid.write(i[0].encode('utf-8')+'\\n')\n",
    "        #print i[0].encode('utf-8'),i[1]\n",
    "    fid.close()\n",
    "def mutijiebaNoEng(date,attrlist,RankNum,title):\n",
    "    filename = 'NE_JiebaTfidf-'+str(RankNum)+'-'+date+'.txt'    \n",
    "    for i in range(0,len(attrlist)):\n",
    "        jiebaNoEng(attrlist[i],RankNum,filename,title)\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "################################################        \n",
    " \n",
    "    \n",
    "#法二\n",
    "def wordsToList(attfile):\n",
    "    import re\n",
    "    f = open(attfile,'r')\n",
    "    f1 = f.read()\n",
    "    f2 = ''.join(f1.split())\n",
    "    words=jieba.cut(f2)\n",
    "    \n",
    "    li = []\n",
    "    for u in words: #把\"有出現兩個連續英數字\"的詞語濾掉\n",
    "        if not re.search('(\\w)(\\w)',u):\n",
    "            li.append(u)\n",
    "    n1 = (' '.join(li)).encode('utf-8')\n",
    "    return n1\n",
    "    f.close()\n",
    "\n",
    "def stopWordsList(stopwordfile):\n",
    "    li = []\n",
    "    fid =  open(stopwordfile,'r')\n",
    "    for line in fid:\n",
    "        li.append(line.strip().decode('utf-8'))#要解開utf-8,才能成功給scikitlearn用\n",
    "    fid.close()\n",
    "    return li\n",
    "\n",
    "def SciNoEng(date,listInput,stopWordsFile,rankNum,title):\n",
    "    from sklearn import feature_extraction  \n",
    "    from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    filename = 'NE_SciTfidf-'+str(rankNum)+'-'+date+'.txt'\n",
    "    \n",
    "    #創造一個空矩陣\n",
    "    corpus = []\n",
    "    \n",
    "    #把傳入的List依序讀出，使用方法把這些東西全都讀出來\n",
    "    for each in range(0,len(listInput)):\n",
    "        corpus.append(wordsToList(listInput[each]))\n",
    "\n",
    "    vectorizer=CountVectorizer(analyzer='word',stop_words=stopWordsList(stopWordsFile))  #該類會將文本中的詞語轉換為詞頻矩陣，矩陣元素a[i][j] 表示j詞在i類文本下的詞頻  \n",
    "    x = vectorizer.fit_transform(corpus)  #將文本轉為詞頻矩陣 \n",
    "    transformer=TfidfTransformer()  #該類會統計每個詞語的tf-idf權值 \n",
    "    tfidf=transformer.fit_transform(x)  #計算tf-idf\n",
    "    word=vectorizer.get_feature_names()  #獲取詞袋模型中的所有詞語\n",
    "    weight=tfidf.toarray()   #將tf-idf矩陣抽取出來，元素a[i][j]表示j詞在i類文本中的tf-idf權重  \n",
    "    \n",
    "    with open(filename,'a') as files:\n",
    "        import operator\n",
    "        for i in range(len(weight)):\n",
    "            \n",
    "            #*#*#*#*#*#*#*#*#*#\n",
    "            #決定是否要印景點標題#\n",
    "            \n",
    "            #print listInput[i]\n",
    "            #files.write(listInput[i])\n",
    "            #files.write('\\n')\n",
    "            \n",
    "            TitleDic=['True','T','t','Yes','Y','y']\n",
    "            if title in TitleDic:\n",
    "                files.write(listInput[i])\n",
    "                files.write('\\n')\n",
    "            else:\n",
    "                files.write('')\n",
    "            \n",
    "            #*#*#*#*#*#*#*#*#*#\n",
    "            dic = {}\n",
    "            for j in range(len(word)):  \n",
    "                dic[word[j]] = weight[i][j]\n",
    "            words_freq = sorted(dic.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "            num2 = 0\n",
    "            for ele in words_freq[0:rankNum]:\n",
    "                num2 += 1\n",
    "                if not ele[1] == float(0.0):\n",
    "                    #print num2,\"\\t\",ele[0],\"\\t\",ele[1]\n",
    "                    files.write(ele[0].encode('utf-8'))\n",
    "                    files.write(\"\\n\")\n",
    "\n",
    "                    \n",
    "################################################  \n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "#法三\n",
    "def jiebafreNoEng(attraction,rankNum,filename,title):\n",
    "#轉換象山資料,並切詞,並使用JIEBA內建的tfidf統計排名    \n",
    "    import re\n",
    "    f = open(attraction,'r')  #讀取檔案\n",
    "    f1 = f.read()  #接住\n",
    "    f2 = ''.join(f1.split())  #把空白全去掉\n",
    "    words=jieba.cut(f2)  #斷詞\n",
    "    \n",
    "    #清洗\n",
    "    li = []\n",
    "    dot = ['。','（','）','！','「','」','，','、','；','：','”','“','～'\\\n",
    "           'Ｐ','Ｓ','．','é','︶','』','『','﹗','ī','ō']\n",
    "    for u in words:\n",
    "        if not re.search('(\\w)(\\w)',u): #清除連續兩個英文字\n",
    "            if not re.match('([#%@$.,/!()-;~&\\'\\\"]|[\\w]|[*^?><])',u): #清除標點符號和單一英文字符\n",
    "                if u.encode('utf-8') not in dot:  #清洗全形符號\n",
    "                    li.append(u)\n",
    "    \n",
    "    n1 = (' '.join(li)).encode('utf-8')\n",
    "    f.close()\n",
    "    x = jieba.analyse.extract_tags(n1, topK=rankNum, withWeight=True)    \n",
    "    fid = open(filename,'a')\n",
    "    \n",
    "    #統計字頻\n",
    "    dic={}\n",
    "    for w in n1.split(' '):\n",
    "        if w not in dic:\n",
    "            dic[w] = 1\n",
    "        else:\n",
    "            dic[w] = dic[w] + 1\n",
    "    import operator\n",
    "    words_freq = sorted(dic.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "    #print attraction#,\"\\n總詞量：\",len(words_freq)\n",
    "\n",
    "    #*#*#*#*#*#*#*#*#*#\n",
    "    #決定是否要印景點標題#\n",
    "            \n",
    "    #print attraction\n",
    "    #fid.write(attraction)\n",
    "    #fid.write('\\n')\n",
    "    \n",
    "    TitleDic=['True','T','t','Yes','Y','y']\n",
    "    if title in TitleDic:\n",
    "        fid.write(attraction)\n",
    "        fid.write('\\n')\n",
    "    else:\n",
    "        fid.write('')\n",
    "    \n",
    "    #*#*#*#*#*#*#*#*#*#\n",
    "    \n",
    "    \n",
    "    #列出原始字頻排名\n",
    "    #print \"==========字頻統計==========\"\n",
    "    #print \"排名\",\"\\t\",\"詞彙\",\"\\t\",\"字頻\"\n",
    "    #print \"- - - - - - - - - - - - - - \"\n",
    "    num2 = 0\n",
    "    for ele in words_freq[0:rankNum]:\n",
    "        num2 += 1\n",
    "        #print num2,\"\\t\",ele[0],\"\\t\",ele[1]\n",
    "        #print ele[0]\n",
    "        #print type(ele[0])\n",
    "        #x = ele[0].encode('utf-8')\n",
    "        fid.write(ele[0])        \n",
    "        fid.write('\\n')\n",
    "    fid.close()\n",
    "#        if num2 % 10 == 0:\n",
    "            #print \"- - - - - - - - - - - - - - \"\n",
    "            \n",
    "# 比對多個景點斷詞字頻的方法\n",
    "def mutifreNoEng(date,attrlist,RankNum,title):\n",
    "    filename = 'NE_WordsFre-'+str(RankNum)+'-'+date+'.txt'  \n",
    "    for i in range(0,len(attrlist)):\n",
    "        jiebafreNoEng(attrlist[i],RankNum,filename,title)\n",
    "\n",
    "        \n",
    "################################################  \n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "#法四\n",
    "def stopWordsSet(stopwordfile):\n",
    "    li = []\n",
    "    fid =  open(stopwordfile,'r')\n",
    "    for line in fid:\n",
    "        li.append(line.strip())\n",
    "    fid.close()\n",
    "    setli = set(li)\n",
    "    return setli\n",
    "\n",
    "def jiebafrestpNoEng(attraction,stopwordfile,rankNum,filename,title):\n",
    "    import re\n",
    "    f = open(attraction,'r')  #讀取檔案\n",
    "    f1 = f.read()  #接住\n",
    "    f2 = ''.join(f1.split())  #把空白全去掉\n",
    "    words=jieba.cut(f2)  #斷詞\n",
    "    \n",
    "    li = []\n",
    "    dot = ['。','（','）','！','「','」','，','、','；','：','”','“','～',\\\n",
    "           'Ｐ','Ｓ','．','é','︶','』','『','﹗','ī','ō','Ｉ','Ｍ']\n",
    "    dot = set(dot)\n",
    "    for u in words:\n",
    "        if not re.search('(\\w)(\\w)',u):\n",
    "            if not re.search('([#%@$.,/!()-;~&\\'\\\"]|[\\w]|[*^?><])',u):\n",
    "                if u.encode('utf-8') not in dot:\n",
    "                    li.append(u)\n",
    "    \n",
    "    n1 = (' '.join(li)).encode('utf-8')\n",
    "    f.close()\n",
    "    \n",
    "    fid = open(filename,'a')\n",
    "    \n",
    "    #統計字頻\n",
    "    dic={}\n",
    "    comparelist = stopWordsSet(stopwordfile)\n",
    "    for w in n1.split(' '):\n",
    "        if w not in comparelist:\n",
    "            if w not in dic:\n",
    "                dic[w] = 1\n",
    "            else:\n",
    "                dic[w] = dic[w] + 1\n",
    "    import operator\n",
    "    words_freq = sorted(dic.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    #*#*#*#*#*#*#*#*#*#\n",
    "    #決定是否要印景點標題#\n",
    "            \n",
    "    #print attraction\n",
    "    #fid.write(attraction)\n",
    "    #fid.write('\\n')\n",
    "    \n",
    "    TitleDic=['True','T','t','Yes','Y','y']\n",
    "    if title in TitleDic:\n",
    "        fid.write(attraction)\n",
    "        fid.write('\\n')\n",
    "    else:\n",
    "        fid.write('')\n",
    "    \n",
    "    #*#*#*#*#*#*#*#*#*#\n",
    "           \n",
    "    num2 = 0\n",
    "    for ele in words_freq[0:rankNum]:\n",
    "        num2 += 1\n",
    "        fid.write(ele[0])\n",
    "        fid.write('\\n')\n",
    "    fid.close()        \n",
    "# 比對多個景點斷詞字頻(去停用詞後)的方法\n",
    "def mutifrestpNoEng(date,attrlist,stopWordsFile,RankNum,title):\n",
    "    filename = 'NE_WordsFreAddStop-'+str(RankNum)+'-'+date+'.txt' \n",
    "    for i in range(0,len(attrlist)):\n",
    "        jiebafrestpNoEng(attrlist[i],stopWordsFile,RankNum,filename,title)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################  \n",
    "\n",
    "################################################\n",
    "#綜合比對\n",
    "\n",
    "\n",
    "def FourWayPK(folder,date,rank,stpDic,title):\n",
    "    #實作：\n",
    "    import time\n",
    "    import os\n",
    "    lis = []\n",
    "    for name in os.listdir(folder):\n",
    "        path = folder+'\\\\'+name\n",
    "        lis.append(path)\n",
    "\n",
    "    x = time.time()\n",
    "\n",
    "    mutijiebaNoEng(date,lis,rank,title)\n",
    "\n",
    "    SciNoEng(date,lis,stpDic,rank,title)\n",
    "\n",
    "    mutifreNoEng(date,lis,rank,title)\n",
    "\n",
    "    mutifrestpNoEng(date,lis,stpDic,rank,title)\n",
    "\n",
    "    y = time.time()\n",
    "    print 'Finish!'\n",
    "    print 'Cost ',y-x,' secs.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 綜合比對 方法\n",
    "使用說明：\n",
    "\n",
    "    FourWayPK(資料夾,日期,排名,停用詞字典,標題)\n",
    "    \n",
    "    資料夾：[字串]。資料夾名稱，內含所有以景點為單位的TXT檔(命名要用英文)\n",
    "           EX:有一'AttrSample'的資料夾，內有30個檔，\n",
    "              每個檔都是一個單獨景點的所有評論\\遊記。\n",
    "              \n",
    "    日　期：[字串]。輸入今天日期，格式-->'YYYYMMDD'\n",
    "           EX:'20151223'\n",
    "           \n",
    "    排　名：[整數]。想要顯示各方法的前幾個排名數\n",
    "           EX:10\n",
    "           \n",
    "    停用詞：[字串]。TXT檔。內容格式需為一行一個字詞，字詞後不得有逗點\n",
    "           EX:'stopwords_1222.txt'\n",
    "           \n",
    "    標　題：[字串]。選擇是否顯示標題(各景點名稱)\n",
    "           傳入：'T'　顯示\n",
    "           　　　'F'　不顯示，檔案裡只會有全部詞語\n",
    "\n",
    "方法結果：\n",
    "\n",
    "    會在所在的資料夾產生四個TXT檔，開頭分別為：\n",
    "    1.NE_JiebaTfidf　　　　-->單一TFIDF\n",
    "    2.NE_SciTfidf         -->多重TFIDF\n",
    "    3.NE_WordsFre         -->普通字頻統計\n",
    "    4.NE_WordsFreAddStop  -->去停用詞字頻統計\n",
    "    \n",
    "    分別是四個方法的結果，並且會依照景點排序。\n",
    "    \n",
    "    且會傳回所耗時數！\n",
    "    \n",
    "# 【注意】每次使用的＂日期＂都要設不一樣\n",
    "# 　　　　否則會在同一個檔案一直append下去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish!\n",
      "Cost  30.896999836  secs.\n"
     ]
    }
   ],
   "source": [
    "FourWayPK('AttrSample100','20160106',30,'stopwords_0104_test.txt','T')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
